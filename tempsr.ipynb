{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7023773,"sourceType":"datasetVersion","datasetId":4039118},{"sourceId":7049357,"sourceType":"datasetVersion","datasetId":4056746},{"sourceId":7056450,"sourceType":"datasetVersion","datasetId":4061742},{"sourceId":7056454,"sourceType":"datasetVersion","datasetId":4061745}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Original code","metadata":{}},{"cell_type":"markdown","source":"import tensorflow as tf\nstrategy = tf.distribute.MirroredStrategy()\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import expand_dims\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Reshape\nfrom keras.layers import Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Add\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Concatenate\nfrom keras.layers import GaussianNoise\nfrom keras.layers import BatchNormalization\nfrom keras.layers import LayerNormalization\nfrom keras.layers import Conv3D\nfrom keras.layers import ConvLSTM3D\nfrom keras.layers import ConvLSTM2D\nfrom keras.layers import TimeDistributed\nfrom keras.initializers import RandomNormal\nimport keras.backend as K\nfrom sklearn.utils import shuffle\n\ndef define_low_res_generator(low_res=(24, 12, 12,3)):\n    #KERNEL INItialization\n    init = RandomNormal(mean=0.0, stddev=0.02)\n    #Image_Input\n    in_img = Input(shape=low_res)\n    #First Conv3d layer\n    fir_gen =Conv3D(64, (3,3,3), strides=(1,1,1), padding='same',data_format='channels_last')(in_img)\n    fir_gen = BatchNormalization(synchronized=False)(fir_gen)\n    fir_gen = LeakyReLU(alpha=0.2)(fir_gen)\n    #Second Conv3d layer\n    gen = Conv3D(64, (3,3,3), strides=(1,1,1), padding='same',data_format='channels_last')(fir_gen)\n    gen = BatchNormalization(synchronized=False)(gen)\n    gen = LeakyReLU(alpha=0.2)(gen)\n    #add noise\n    gen=GaussianNoise(0.01)(gen)\n    #ConvLstm layer for aggregating weather parameters from /hr to /day\n    Conv_layer = ConvLSTM2D(128,(3,3), activation =\"tanh\", padding='same',data_format='channels_last', name='LSTMLayer')(gen)\n    # final conv2d layer to generate low resolution output\n    out_layer= Conv2D(1, (3,3), strides=(1,1), padding='same',data_format='channels_last')(Conv_layer)\n    #define the model with it's input and output\n    model = Model(inputs=in_img, outputs=out_layer, name=\"low_res_generator\")\n    #generate a model summary \n    #model.summary()\n    return model\n\n\ndef define_discriminator(in_shape=(48,48,1), n_class=5):\n    # label input\n    in_label = Input(shape=(1,))\n    # embedding the label input\n    li = Embedding(n_class, 50)(in_label)\n    # scale up to image dimensions with linear activation\n    n_nodes = in_shape[0]*in_shape[1]\n    li = Dense(n_nodes)(li)\n    # reshape to additional channel\n    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n    #KERNEL INItialization\n    init = RandomNormal(mean=0.0, stddev=0.02)\n    # image input\n    in_img = Input(shape=in_shape)\n    in_image=Concatenate()([in_img,li])\n    #add a convolutional layers\n    conv1 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(in_image)\n    conv1 = LayerNormalization()(conv1)\n    conv1 = LeakyReLU(alpha=0.2)(conv1)\n    # add 1st residual layer to the discriminator\n    res11 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(conv1)\n    res11 = LayerNormalization()(res11)\n    res11 = LeakyReLU(alpha=0.2)(res11)\n    # add 2nd residual layer to the discriminator\n    res12 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res11)\n    res12 = LayerNormalization()(res12)\n    res12 = Add()([res12, conv1])\n    res12 = LeakyReLU(alpha=0.2)(res12)\n    # add 1st residual layer to the discriminator\n    res21 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res12)\n    res21 = LayerNormalization()(res21)\n    res21 = LeakyReLU(alpha=0.2)(res21)\n    # add 2nd residual layer to the discriminator\n    res22 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res21)\n    res22 = LayerNormalization()(res22)\n    res22 = Add()([res22, res12])\n    res22 = LeakyReLU(alpha=0.2)(res22)\n    # add noise\n    res22=GaussianNoise(0.01)(res22)\n    # downsample layer 1\n    conv2 = Conv2D(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(res22)\n    conv2 = LayerNormalization()(conv2)\n    conv2 = LeakyReLU(alpha=0.2)(conv2)\n    # downsample layer 2\n    conv3 = Conv2D(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(conv2)\n    conv3 = LayerNormalization()(conv3)\n    conv3 = LeakyReLU(alpha=0.2)(conv3)\n    # downsample layer 3\n    conv4 = Conv2D(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(conv3)\n    conv4 = LayerNormalization()(conv4)\n    conv4 = LeakyReLU(alpha=0.2)(conv4)  \n    # flatten feature maps\n    fl = Flatten()(conv4)\n    # dropout\n    fl = Dropout(0.4)(fl)\n    # output\n    out_layer = Dense(1)(fl)\n    #define the model with it's input and output\n    model = Model(inputs=[in_img,in_label], outputs= out_layer, name=\"discriminator\")\n    #generate a model summary \n    #model.summary()\n    return model\n\n# define the standalone generator model\ndef define_generator(low_res=(12,12,128)):\n    #KERNEL INItialization\n    init = RandomNormal(mean=0.0, stddev=0.02)\n    in_img = Input(shape=low_res)\n    # add 1st residual layer to the generator\n    res11 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(in_img)\n    res11 = LayerNormalization()(res11)\n    res11 = LeakyReLU(alpha=0.2)(res11)\n    # add 2nd residual layer to the generator\n    res12 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res11)\n    res12 = LayerNormalization()(res12)\n    res12 = Add()([res12, in_img])\n    res12 = LeakyReLU(alpha=0.2)(res12)\n    # add 1st residual layer to the generator\n    res21 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res12)\n    res21 = LayerNormalization()(res21)\n    res21 = LeakyReLU(alpha=0.2)(res21)\n    # add 2nd residual layer to the generator\n    res22 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res21)\n    res22 = LayerNormalization()(res22)\n    res22 = Add()([res22, res12])\n    res22 = LeakyReLU(alpha=0.2)(res22)\n    # add noise\n    res22=GaussianNoise(0.01)(res22)\n    # upsampling to 24x24\n    convt1 = Conv2DTranspose(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(res22)\n    convt1 = BatchNormalization(synchronized=False)(convt1)\n    convt1 = LeakyReLU(alpha=0.2)(convt1)\n    #2nd upsampling to 48x48\n    convt2 = Conv2DTranspose(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(convt1)\n    convt2 = BatchNormalization(synchronized=False)(convt2)\n    convt2 = LeakyReLU(alpha=0.2)(convt2)\n    # output\n    out_layer = Conv2D(1, (5,5), activation='tanh', padding='same',kernel_initializer=init)(convt2)\n    #define the model with it's input and output\n    model = Model(inputs=in_img, outputs=out_layer, name=\"generator\")\n    #generate a model summary \n    #model.summary()\n    return model\n\ndef define_combined_generator(low_res_gen,generator): \n    # input to the corrector generator\n    in_img = low_res_gen.input \n    # get image output from the corrector \n    low_res_output = low_res_gen.get_layer('LSTMLayer').output \n    # connect image output to the generator\n    out_layer = generator(low_res_output) \n    # define the combined model \n    model = Model(inputs=in_img , outputs= out_layer, name= 'Combined_generator')  \n    return model\n\nclass WGAN(keras.Model):\n    def __init__(self, discriminator, generator, Dsteps=5, gp_weight=10.0):\n        super(WGAN, self).__init__()\n\n        self.discriminator = discriminator\n        self.generator = generator\n        self.d_steps = Dsteps\n        self.gp_weight = gp_weight\n\n    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n        super(WGAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_fn = d_loss_fn\n        self.g_loss_fn = g_loss_fn\n    \n    \n    def gradient_penalty (self,batch_size, real_images, fake_images, labels):\n        alpha = tf.random.uniform([batch_size, 1, 1, 1], minval=0.,maxval=1.)\n        diff = fake_images - real_images\n        interpolated = real_images + alpha * diff\n\n        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            pred = self.discriminator([interpolated,labels], training=True)\n\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        gp = tf.reduce_mean((norm - 1.0) ** 2)\n        return gp                             \n\n    def train_step(self, data):\n        #if isinstance(data, list):\n        lowres_images = data[0]\n        real_images=data[1][0]\n        labels=data[1][1]\n        batch_size = tf.shape(real_images)[0]\n\n        for i in range(self.d_steps):\n            with tf.GradientTape() as tape:\n                # Generate fake images from the latent vector\n                fake_images = self.generator(lowres_images, training=True)\n                # Get the logits for the fake images\n                fake_logits = self.discriminator([fake_images,labels], training=True)\n                # Get the logits for the real images\n                real_logits = self.discriminator([real_images,labels], training=True)\n                # Calculate the discriminator loss using the fake and real image logits\n                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n                # Calculate the gradient penalty\n                gp = self.gradient_penalty(batch_size, real_images, fake_images,labels)\n                # Add the gradient penalty to the original discriminator loss\n                d_loss = d_cost + gp * self.gp_weight\n                # Get the gradients w.r.t the discriminator loss\n                d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n                # Update the weights of the discriminator using the discriminator optimizer\n                self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n        # Train the generator\n        #lowres_images= self.generate_low_res_samples(real_images)\n        with tf.GradientTape() as tape:\n            # Generate fake images using the generator\n            generated_images = self.generator(lowres_images, training=True)\n            # Get the discriminator logits for fake images\n            gen_img_logits = self.discriminator([generated_images,labels], training=True)\n            # Calculate the generator loss\n            g_loss = self.g_loss_fn(gen_img_logits)\n        # Get the gradients w.r.t the generator loss\n        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n        # Update the weights of the generator using the generator optimizer\n        self.g_optimizer.apply_gradients(zip(gen_gradient, self.generator.trainable_variables))\n        return {\"d_loss\": d_loss, \"g_loss\": g_loss, 'batch_size':batch_size}\n\n    \n\n# define a callback to save keras model after every epoch      \nclass GANMonitor(keras.callbacks.Callback):\n    def __init__(self, gen_loss, critic_loss):\n        self.gen_loss=gen_loss\n        self.critic_loss=critic_loss\n    \n    def on_epoch_end(self, epoch, logs={}):\n        g_model.save('Pretraining_2_3Channels%.1f.keras'%epoch) \n        self.gen_loss.append(logs.get('g_loss'))\n        self.critic_loss.append(logs.get('d_loss'))\n        \n# list to track the losses for training the WGAN-GP   \ngen_loss=[]\ncritic_loss=[]\n# tensorflow method to call all available gpu's for training.\nwith strategy.scope():\n    # creating the model architecture\n    cbk = GANMonitor(gen_loss, critic_loss)\n    pretraining=define_low_res_generator()\n    generator=define_generator()\n    g_model=define_combined_generator(pretraining,generator)\n    d_model= define_discriminator()\n    \n    #define the optimizer for generator and discriminator\n    pretraining_optimizer= keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    g_model_optimizer= keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    generator_optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    \n    #define the pretraining loss function\n    def corrector_loss(y_true, y_pred):\n        #calculating the soft discritization FSS score with coutoff 0.5 on images scaled [0,1]\n        gamma=0.1\n        c=10\n        cutoff=0.5\n        eps = K.epsilon()\n        y_true_bi = tf.math.sigmoid( c * ( y_true - cutoff ))\n        y_pred_bi = tf.math.sigmoid( c * ( y_pred - cutoff ))\n        MSE_n = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(y_true_bi, y_pred_bi) \n        #MSE_weighted(y_true_bi,y_pred_bi) \n        #tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(y_true_bi, y_pred_bi)\n        O_sqimg = tf.keras.layers.Multiply()([y_true_bi, y_true_bi])   \n        O_sqvec = tf.keras.layers.Flatten()(O_sqimg)\n        M_sqimg = tf.keras.layers.Multiply()([y_pred_bi, y_pred_bi])\n        M_sqvec = tf.keras.layers.Flatten()(M_sqimg)\n        MSE_ref = tf.math.reduce_mean(O_sqvec + M_sqvec)\n        return (tf.math.reduce_mean(tf.keras.losses.huber(y_true, y_pred, delta=0.1)+ gamma*(float(MSE_n) / float(MSE_ref+eps))))\n    \n    def MSE_weighted(y_true,y_pred):\n        return K.mean(tf.multiply(tf.square(y_true),tf.square(tf.subtract(y_pred, y_true))))\n\n    def gen_fss(y_true, y_pred):\n        #calculating the soft discritization FSS score with coutoff 0.5 on images scaled [0,1]\n        gamma=10\n        c=10\n        cutoff=0.5\n        eps = K.epsilon()\n        y_true_bi = tf.math.sigmoid( c * ( y_true - cutoff ))\n        y_pred_bi = tf.math.sigmoid( c * ( y_pred - cutoff ))\n        MSE_n =MSE_weighted(y_true_bi,y_pred_bi) \n        #MSE_weighted(y_true_bi,y_pred_bi) \n        #tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(y_true_bi, y_pred_bi)\n        MSE_n=tf.cast(MSE_n, tf.float32)\n        O_sqimg = tf.keras.layers.Multiply()([y_true_bi, y_true_bi])   \n        O_sqvec = tf.keras.layers.Flatten()(O_sqimg)\n        M_sqimg = tf.keras.layers.Multiply()([y_pred_bi, y_pred_bi])\n        M_sqvec = tf.keras.layers.Flatten()(M_sqimg)\n        MSE_ref = tf.math.reduce_mean(O_sqvec + M_sqvec)\n        MSE_ref=tf.cast(MSE_ref, tf.float32)\n        return (tf.math.reduce_mean(tf.keras.losses.huber(y_true, y_pred, delta=0.1)+ gamma*(float(MSE_n) / float(MSE_ref+eps))))\n                \n    #critic loss without the gradient penalty ter\n    def discriminator_loss(real_img, fake_img):\n        real_loss = tf.reduce_mean(real_img)\n        fake_loss = tf.reduce_mean(fake_img)\n        return fake_loss - real_loss\n    \n    #generator loss \n    def generator_loss(fake_img):\n        fake_loss= -tf.reduce_mean(fake_img)\n        return fake_loss\n    \n    #compile the pretraining model using low_res_gen\n    pretraining.compile(optimizer=pretraining_optimizer, loss= corrector_loss)        \n    g_model.compile(optimizer=g_model_optimizer, loss=gen_fss)\n    #define the model with generator and critic\n    wgan = WGAN(discriminator=d_model, generator=g_model, Dsteps=5)\n    # Compile the WGAN model.\n    wgan.compile(d_optimizer=discriminator_optimizer, g_optimizer=generator_optimizer, g_loss_fn=generator_loss, d_loss_fn=discriminator_loss,)       \n\n#create a low resolution of the observational data to feed the pre-training   \ndef Pooling(High_Res_Data):\n    Avgpool= MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')\n    low1=Avgpool(High_Res_Data)\n    low2=Avgpool(low1)\n    return low2\n\n#load the data\nInput_data=np.load(\"/kaggle/input/threechannelinput/Inputfile3channels.npy\")\nHighres_data=np.load(\"/kaggle/input/threechannelinput/Highresfile3channels.npy\")\nlabels_data=np.load(\"/kaggle/input/threechannelinput/labelfile3channels.npy\")\nLow_Res_Data, High_Res_Data, labels= shuffle(Input_data, Highres_data, labels_data)\nprint(Low_Res_Data.shape,High_Res_Data.shape, labels_data.shape)\n               \n#define the batch size and epochs\npre_epoch=10\npre_batch_size=128\ntraining_epoch =32\ntraining_batch_size=64\ntotal_samples=High_Res_Data.shape[0]\n\n#maintain consistent number of samples per epoch\nstpe_wgan=total_samples//training_batch_size\nstpe_pretrain=total_samples//pre_batch_size\n#create a low resolution of the observational data set\nMaxpooled_data=Pooling(High_Res_Data)\n\n#Train the model.\npretraining.fit(Low_Res_Data, Maxpooled_data,batch_size=pre_batch_size,epochs=pre_epoch,steps_per_epoch=stpe_pretrain, verbose=2)\ng_model.fit(Low_Res_Data, High_Res_Data, batch_size=pre_batch_size,epochs=pre_epoch, steps_per_epoch=stpe_pretrain, verbose=2)\nwgan.fit(Low_Res_Data, [High_Res_Data,labels], batch_size=training_batch_size, epochs=training_epoch,callbacks=[cbk],steps_per_epoch=stpe_wgan, verbose=2)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Edit in progress below","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nstrategy = tf.distribute.MirroredStrategy()\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import expand_dims\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Reshape\nfrom keras.layers import Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Add\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Concatenate\nfrom keras.layers import GaussianNoise\nfrom keras.layers import BatchNormalization\nfrom keras.layers import LayerNormalization\nfrom keras.layers import Conv3D\nfrom keras.layers import ConvLSTM3D\nfrom keras.layers import ConvLSTM2D\nfrom keras.layers import TimeDistributed\nfrom keras.initializers import RandomNormal\nimport keras.backend as K\nfrom sklearn.utils import shuffle\n\ndef define_low_res_generator(low_res=(12, 12, 1)):\n    #KERNEL INItialization\n    init = RandomNormal(mean=0.0, stddev=0.02)\n    #Image_Input\n    in_img = Input(shape=low_res)\n    #add a convolutional layers\n    conv1 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(in_img)\n    conv1 = LayerNormalization()(conv1)\n    conv1 = LeakyReLU(alpha=0.2)(conv1)\n    #residual layer\n    res11 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(conv1)\n    res11 = LayerNormalization()(res11)\n    res11 = LeakyReLU(alpha=0.2)(res11)\n    # add 2nd residual layer to the discriminator\n    res12 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res11)\n    res12 = LayerNormalization()(res12)\n    res12 = Add()([res12, conv1])\n    res12 = LeakyReLU(alpha=0.2)(res12)\n    #add noise\n    res12=GaussianNoise(0.02)(res12)\n    # add 1st residual layer to the discriminator\n    res21 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res12)\n    res21 = LayerNormalization()(res21)\n    res21 = LeakyReLU(alpha=0.2)(res21)\n    # add 2nd residual layer to the discriminator\n    res22 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res21)\n    res22 = LayerNormalization()(res22)\n    res22 = Add()([res22, res12])\n    gen_layer = LeakyReLU(alpha=0.2, name=\"gen_layer\")(res22)\n    # final conv2d layer to generate low resolution output\n    out_layer= Conv2D(1, (3,3), strides=(1,1), activation='tanh',padding='same')(gen_layer)\n    #define the model with it's input and output\n    model = Model(inputs=in_img, outputs=out_layer, name=\"low_res_generator\")\n    #generate a model summary \n    #model.summary()\n    return model\n\n\ndef define_discriminator(in_shape=(48,48,1)):\n    # label input\n    #in_label = Input(shape=(1,))\n    # embedding the label input\n    #li = Embedding(n_class, 50)(in_label)\n    # scale up to image dimensions with linear activation\n    #n_nodes = in_shape[0]*in_shape[1]\n    #li = Dense(n_nodes)(li)\n    # reshape to additional channel\n    #li = Reshape((in_shape[0], in_shape[1], 1))(li)\n    #KERNEL INItialization\n    init = RandomNormal(mean=0.0, stddev=0.02)\n    # image input\n    in_image = Input(shape=in_shape)\n    #in_image=Concatenate()([in_img,li])\n    #add a convolutional layers\n    conv1 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(in_image)\n    conv1 = LayerNormalization()(conv1)\n    conv1 = LeakyReLU(alpha=0.2)(conv1)\n    # add 1st residual layer to the discriminator\n    res11 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(conv1)\n    res11 = LayerNormalization()(res11)\n    res11 = LeakyReLU(alpha=0.2)(res11)\n    # add 2nd residual layer to the discriminator\n    res12 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res11)\n    res12 = LayerNormalization()(res12)\n    res12 = Add()([res12, conv1])\n    res12 = LeakyReLU(alpha=0.2)(res12)\n    # add 1st residual layer to the discriminator\n    res21 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res12)\n    res21 = LayerNormalization()(res21)\n    res21 = LeakyReLU(alpha=0.2)(res21)\n    # add 2nd residual layer to the discriminator\n    res22 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res21)\n    res22 = LayerNormalization()(res22)\n    res22 = Add()([res22, res12])\n    res22 = LeakyReLU(alpha=0.2)(res22)\n    # add noise\n    res22=GaussianNoise(0.01)(res22)\n    # downsample layer 1\n    conv2 = Conv2D(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(res22)\n    conv2 = LayerNormalization()(conv2)\n    conv2 = LeakyReLU(alpha=0.2)(conv2)\n    # downsample layer 2\n    conv3 = Conv2D(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(conv2)\n    conv3 = LayerNormalization()(conv3)\n    conv3 = LeakyReLU(alpha=0.2)(conv3)\n    # downsample layer 3\n    conv4 = Conv2D(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(conv3)\n    conv4 = LayerNormalization()(conv4)\n    conv4 = LeakyReLU(alpha=0.2)(conv4)  \n    # flatten feature maps\n    fl = Flatten()(conv4)\n    # dropout\n    fl = Dropout(0.3)(fl)\n    # output\n    out_layer = Dense(1)(fl)\n    #define the model with it's input and output\n    model = Model(inputs=in_image, outputs= out_layer, name=\"discriminator\")\n    #generate a model summary \n    #model.summary()\n    return model\n\n# define the standalone generator model\ndef define_generator(low_res=(12,12,128)):\n    #KERNEL INItialization\n    init = RandomNormal(mean=0.0, stddev=0.02)\n    in_img = Input(shape=low_res)\n    # add 1st residual layer to the generator\n    res11 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(in_img)\n    res11 = LayerNormalization()(res11)\n    res11 = LeakyReLU(alpha=0.2)(res11)\n    # add 2nd residual layer to the generator\n    res12 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res11)\n    res12 = LayerNormalization()(res12)\n    res12 = Add()([res12, in_img])\n    res12 = LeakyReLU(alpha=0.2)(res12)\n    # add 1st residual layer to the generator\n    res21 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res12)\n    res21 = LayerNormalization()(res21)\n    res21 = LeakyReLU(alpha=0.2)(res21)\n    # add 2nd residual layer to the generator\n    res22 = Conv2D(128, (3,3), strides=(1,1), padding='same',kernel_initializer=init)(res21)\n    res22 = LayerNormalization()(res22)\n    res22 = Add()([res22, res12])\n    res22 = LeakyReLU(alpha=0.2)(res22)\n    # add noise\n    res22=GaussianNoise(0.01)(res22)\n    # upsampling to 24x24\n    convt1 = Conv2DTranspose(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(res22)\n    convt1 = LayerNormalization()(convt1)\n    convt1 = LeakyReLU(alpha=0.2)(convt1)\n    #2nd upsampling to 48x48\n    convt2 = Conv2DTranspose(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init)(convt1)\n    convt2 = LayerNormalization()(convt2)\n    convt2 = LeakyReLU(alpha=0.2)(convt2)\n    # output\n    out_layer = Conv2D(1, (5,5), activation='tanh', padding='same',kernel_initializer=init)(convt2)\n    #define the model with it's input and output\n    model = Model(inputs=in_img, outputs=out_layer, name=\"generator\")\n    #generate a model summary \n    #model.summary()\n    return model\n\ndef define_combined_generator(low_res_gen,generator): \n    # input to the corrector generator\n    in_img = low_res_gen.input \n    # get image output from the corrector \n    low_res_output = low_res_gen.get_layer(\"gen_layer\").output \n    # connect image output to the generator\n    out_layer = generator(low_res_output) \n    # define the combined model \n    model = Model(inputs=in_img , outputs= out_layer, name= 'Combined_generator')  \n    return model\n\nclass WGAN(keras.Model):\n    def __init__(self, discriminator, generator, Dsteps=5, gp_weight=10.0):\n        super(WGAN, self).__init__()\n\n        self.discriminator = discriminator\n        self.generator = generator\n        self.d_steps = Dsteps\n        self.gp_weight = gp_weight\n\n    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n        super(WGAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_fn = d_loss_fn\n        self.g_loss_fn = g_loss_fn\n    \n    \n    def gradient_penalty (self,batch_size, real_images, fake_images):\n        alpha = tf.random.uniform([batch_size, 1, 1, 1], minval=0.,maxval=1.)\n        diff = fake_images - real_images\n        interpolated = real_images + alpha * diff\n\n        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            pred = self.discriminator(interpolated, training=True)\n\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        gp = tf.reduce_mean((norm - 1.0) ** 2)\n        return gp                             \n\n    def train_step(self, data):\n        #if isinstance(data, list):\n        lowres_images = data[0]\n        real_images=data[1]\n        batch_size = tf.shape(real_images)[0]\n\n        for i in range(self.d_steps):\n            with tf.GradientTape() as tape:\n                # Generate fake images from the latent vector\n                fake_images = self.generator(lowres_images, training=True)\n                # Get the logits for the fake images\n                fake_logits = self.discriminator(fake_images, training=True)\n                # Get the logits for the real images\n                real_logits = self.discriminator(real_images, training=True)\n                # Calculate the discriminator loss using the fake and real image logits\n                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n                # Calculate the gradient penalty\n                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n                # Add the gradient penalty to the original discriminator loss\n                d_loss = d_cost + gp * self.gp_weight\n                # Get the gradients w.r.t the discriminator loss\n                d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n                # Update the weights of the discriminator using the discriminator optimizer\n                self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n        # Train the generator\n        #lowres_images= self.generate_low_res_samples(real_images)\n        with tf.GradientTape() as tape:\n            # Generate fake images using the generator\n            generated_images = self.generator(lowres_images, training=True)\n            # Get the discriminator logits for fake images\n            gen_img_logits = self.discriminator(generated_images, training=True)\n            # Calculate the generator loss\n            g_loss = self.g_loss_fn(gen_img_logits)\n        # Get the gradients w.r.t the generator loss\n        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n        # Update the weights of the generator using the generator optimizer\n        self.g_optimizer.apply_gradients(zip(gen_gradient, self.generator.trainable_variables))\n        return {\"d_loss\": d_loss, \"g_loss\": g_loss, 'batch_size':batch_size}\n\n    \n\n# define a callback to save keras model after every epoch      \nclass GANMonitor(keras.callbacks.Callback):\n    def __init__(self, gen_loss, critic_loss):\n        self.gen_loss=gen_loss\n        self.critic_loss=critic_loss\n    \n    def on_epoch_end(self, epoch, logs={}):\n        g_model.save('TempSRgencomb%.1f.keras'%epoch)\n        d_model.save('TempSRdisc%.1f.keras'%epoch)\n        pretraining.save('TempSRpretrain%.1f.keras'%epoch)\n       # model.save('./MyModel_tf',save_format='tf')\n        self.gen_loss.append(logs.get('g_loss'))\n        self.critic_loss.append(logs.get('d_loss'))\n        \n# list to track the losses for training the WGAN-GP   \ngen_loss=[]\ncritic_loss=[]\n# tensorflow method to call all available gpu's for training.\nwith strategy.scope():\n    # creating the model architecture\n    cbk = GANMonitor(gen_loss, critic_loss)\n    pretraining=define_low_res_generator()\n    generator=define_generator()\n    g_model=define_combined_generator(pretraining,generator)\n    d_model= define_discriminator()\n    \n    #define the optimizer for generator and discriminator\n    pretraining_optimizer= keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    generator_optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    \n    def MSE_weighted(y_true,y_pred):\n        return K.mean(tf.multiply(tf.square(y_true),tf.square(tf.subtract(y_pred, y_true))))\n\n    #define the pretraining loss function\n    def corrector_loss(y_true, y_pred):\n        #calculating the soft discritization FSS score with coutoff 0.5 on images scaled [0,1]\n        gamma=0.1\n        c=10\n        cutoff=0.5\n        eps = K.epsilon()\n        y_true_bi = tf.math.sigmoid( c * ( y_true - cutoff ))\n        y_pred_bi = tf.math.sigmoid( c * ( y_pred - cutoff ))\n        MSE_n = MSE_weighted(y_true_bi,y_pred_bi) \n        #tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(y_true_bi, y_pred_bi)\n        O_sqimg = tf.keras.layers.Multiply()([y_true_bi, y_true_bi])   \n        O_sqvec = tf.keras.layers.Flatten()(O_sqimg)\n        M_sqimg = tf.keras.layers.Multiply()([y_pred_bi, y_pred_bi])\n        M_sqvec = tf.keras.layers.Flatten()(M_sqimg)\n        MSE_ref = tf.math.reduce_mean(O_sqvec + M_sqvec)\n        return (tf.math.reduce_mean(tf.keras.losses.huber(y_true, y_pred, delta=0.1)+ gamma*(float(MSE_n) / float(MSE_ref+eps))))\n    \n                \n    #critic loss without the gradient penalty ter\n    def discriminator_loss(real_img, fake_img):\n        real_loss = tf.reduce_mean(real_img)\n        fake_loss = tf.reduce_mean(fake_img)\n        return fake_loss - real_loss\n    \n    #generator loss \n    def generator_loss(fake_img):\n        fake_loss= -tf.reduce_mean(fake_img)\n        return fake_loss\n    \n    #compile the pretraining model using low_res_gen\n    pretraining.compile(optimizer=pretraining_optimizer, loss= corrector_loss)        \n    #define the model with generator and critic\n    wgan = WGAN(discriminator=d_model, generator=g_model, Dsteps=5)\n    # Compile the WGAN model.\n    wgan.compile(d_optimizer=discriminator_optimizer, g_optimizer=generator_optimizer, g_loss_fn=generator_loss, d_loss_fn=discriminator_loss,)       \n\n#create a low resolution of the observational data to feed the pre-training   \ndef Pooling(High_Res_Data):\n    Avgpool= MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')\n    low1=Avgpool(High_Res_Data)\n    low2=Avgpool(low1)\n    return low2\n\n#load the data\nInput_data=np.load(\"/kaggle/input/tempimdera/Inputfiletemp.npy\")\nHighres_data=np.load(\"/kaggle/input/tempimdera/Highresfiletemp.npy\")\nLow_Res_Data, High_Res_Data= shuffle(Input_data, Highres_data)\nprint(Low_Res_Data.shape,High_Res_Data.shape)\n               \n#define the batch size and epochs\npre_epoch=10\npre_batch_size=128\ntraining_epoch =20\ntraining_batch_size=128\ntotal_samples=High_Res_Data.shape[0]\n\n#maintain consistent number of samples per epoch\nstpe_wgan=total_samples//training_batch_size\nstpe_pretrain=total_samples//pre_batch_size\n#create a low resolution of the observational data set\nMaxpooled_data=Pooling(High_Res_Data)\n\n#Train the model.\npretraining.fit(Low_Res_Data, Maxpooled_data,batch_size=pre_batch_size,epochs=pre_epoch,steps_per_epoch=stpe_pretrain, verbose=2)\nwgan.fit(Low_Res_Data, High_Res_Data, batch_size=training_batch_size, epochs=training_epoch,callbacks=[cbk],steps_per_epoch=stpe_wgan, verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom numpy import expand_dims\nimport numpy.ma as ma\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.cm as mtpltcm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport random\nimport folium\nfrom folium.plugins import HeatMap, HeatMapWithTime\nfrom folium import plugins\nfrom netCDF4 import Dataset\nimport cartopy.crs as ccrs\nfrom tensorflow import keras\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Add\nfrom keras.models import load_model\nimport numpy.ma as ma\nfrom warnings import filterwarnings\nimport glob\nimport os\n\nINPUT=np.load(\"/kaggle/input/testfile/Inputfiletemptest.npy\")\nOUTPUT=np.load(\"/kaggle/input/testfile/Highresfiletemptest.npy\")\nmodel = load_model(\"/kaggle/input/model1/TempSR11.0.keras\")\nIMAGE=4567\n\ngen=INPUT[IMAGE]\nera=OUTPUT[IMAGE]\nera=np.ma.masked_greater(era, 50)\nera=np.reshape(era, (48,48))\n\ngen=np.reshape(gen, (1, 12, 12, 1))\nOUT = model.predict(gen)\nOUT=np.reshape(OUT,(48,48))\nOUT=np.ma.masked_greater(OUT, 50)\nIN=np.reshape(gen, (12, 12))\nIN=np.ma.masked_greater(IN, 50)\n\nimdlat=np.array([i for i in np.arange(7.5,39.5,1)])\nimdlong=np.array([i for i in np.arange(67.5,99.5,1)])\n\n\n(iln5, ilt5) = (imdlong[5:17], imdlat[0:12])\n(iln2, ilt2) = (imdlong[0:12], imdlat[10:22])\n(iln3, ilt3) = (imdlong[10:22], imdlat[10:22])\n(iln4, ilt4) = (imdlong[20:32], imdlat[12:24])\n(iln1, ilt1) = (imdlong[4:16], imdlat[20:32])\n\nif (IMAGE//1096==0):\n\t(ilat, ilong)=(iln1, ilt1)\nelif (IMAGE//1096==1):\n\t(ilat, ilong)=(iln2, ilt2)\nelif (IMAGE//1096==2):\n\t(ilat, ilong)=(iln3, ilt3)\nelif(IMAGE//1096==3):\n\t(ilat, ilong)=(iln4, ilt4)\nelse:\n\t(ilat, ilong)=(iln5, ilt5)\n\n\neralat=np.array([i for i in np.arange(7,39,0.25)])\neralong=np.array([i for i in np.arange(67.25,99.25,0.25)])\n\n\n(ln5, lt5) = (eralong[20:68], eralat[0:48])\n(ln2, lt2) = (eralong[0:48], eralat[40:88])\n(ln3, lt3) = (eralong[40:88], eralat[40:88])\n(ln4, lt4) = (eralong[80:128], eralat[48:96])\n(ln1, lt1) = (eralong[16:64], eralat[80:128])\n\nif (IMAGE//1096==0):\n\t(elat, elong)=(ln1, lt1)\nelif (IMAGE//1096==1):\n\t(elat, elong)=(ln2, lt2)\nelif (IMAGE//1096==2):\n\t(elat, elong)=(ln3, lt3)\nelif(IMAGE//1096==3):\n\t(elat, elong)=(ln4, lt4)\nelse:\n\t(elat, elong)=(ln5, lt5)\n\nmaxval=np.amax(OUT)\nminval=np.amin(OUT)\nlevelout=np.linspace(minval,maxval, 50)\n\nmaxval=np.amax(era)\nminval=np.amin(era)\nlevelera=np.linspace(minval,maxval, 50)\n\nplt.style.use(\"seaborn-v0_8-bright\")\nfigure = plt.figure(figsize=(10,10))\naxis_func = plt.axes(projection=ccrs.PlateCarree())\naxis_func.coastlines(resolution=\"10m\",linewidth=1)\naxis_func.gridlines(linestyle='--',color='black',linewidth=2)\nN=99\nplt.contourf(elat, elong,  era, N, transform=ccrs.PlateCarree(), cmap='terrain', levels=levelout)\ncolor_bar_func = plt.colorbar(ax=axis_func, orientation=\"vertical\", pad=0.05, aspect=16, shrink=.8)\ncolor_bar_func.ax.tick_params(labelsize=12)\nplt.tight_layout()\n#plt.savefig('dp4.png')\nplt.show()\n\nplt.style.use(\"seaborn-v0_8-bright\")\nfigure = plt.figure(figsize=(10,10))\naxis_func = plt.axes(projection=ccrs.PlateCarree())\naxis_func.coastlines(resolution=\"10m\",linewidth=1)\naxis_func.gridlines(linestyle='--',color='black',linewidth=2)\nN=99\nplt.contourf(elat, elong , OUT, N, transform=ccrs.PlateCarree(), cmap='terrain', levels=levelout)\ncolor_bar_func = plt.colorbar(ax=axis_func, orientation=\"vertical\", pad=0.05, aspect=16, shrink=.8)\ncolor_bar_func.ax.tick_params(labelsize=12)\nplt.tight_layout()\n##plt.savefig('dp4.png')\nplt.show()\n\n\nplt.style.use(\"seaborn-v0_8-bright\")\nfigure = plt.figure(figsize=(10,10))\naxis_func = plt.axes(projection=ccrs.PlateCarree())\naxis_func.coastlines(resolution=\"10m\",linewidth=1)\naxis_func.gridlines(linestyle='--',color='black',linewidth=2)\nN=99\nplt.contourf(ilat, ilong, IN, N, transform=ccrs.PlateCarree(), cmap='terrain',levels=levelout)\ncolor_bar_func = plt.colorbar(ax=axis_func, orientation=\"vertical\", pad=0.05, aspect=16, shrink=.8)\ncolor_bar_func.ax.tick_params(labelsize=12)\nplt.tight_layout()\n#plt.savefig('dp4.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:43:47.362990Z","iopub.execute_input":"2023-11-26T11:43:47.363621Z","iopub.status.idle":"2023-11-26T11:44:11.816979Z","shell.execute_reply.started":"2023-11-26T11:43:47.363576Z","shell.execute_reply":"2023-11-26T11:44:11.815686Z"}}}]}